{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uhPZU2o6Aoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e09879-065e-46c9-ce1f-f74e12256b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.nn.functional import pad\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Function to read your custom dataset\n",
        "def read_telugu_english_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        raw_data = []\n",
        "        for line in file:\n",
        "            telugu_sentence, english_sentence = line.strip().split('++++$++++')\n",
        "            raw_data.append((telugu_sentence, english_sentence))\n",
        "    return raw_data\n",
        "\n",
        "# Splitting the dataset\n",
        "def split_dataset(data, train_split=0.7, val_split=0.15, test_split=0.15):\n",
        "    total_size = len(data)\n",
        "    train_size = int(total_size * train_split)\n",
        "    val_size = int(total_size * val_split)\n",
        "    test_size = total_size - train_size - val_size\n",
        "    train_data, remaining_data = random_split(data, [train_size, total_size - train_size])\n",
        "    val_data, test_data = random_split(remaining_data, [val_size, test_size])\n",
        "    return list(train_data), list(val_data), list(test_data)\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_raw_texts(self):\n",
        "        return [(src, trg) for src, trg in self.data]\n",
        "\n",
        "# Define tokenizers\n",
        "tokenizer_te = get_tokenizer('basic_english')  # Replace with a suitable tokenizer for Telugu\n",
        "tokenizer_en = get_tokenizer('basic_english')  # Suitable tokenizer for English\n",
        "\n",
        "# Build vocabulary function\n",
        "def build_vocabulary(tokenizer, dataset, min_freq=2):\n",
        "    def yield_tokens(data):\n",
        "        for src, trg in data:\n",
        "            yield tokenizer(src)\n",
        "            yield tokenizer(trg)\n",
        "\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(dataset.get_raw_texts()), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], min_freq=min_freq)\n",
        "    vocab.set_default_index(vocab['<unk>'])  # Set default index for unknown tokens\n",
        "    return vocab\n",
        "\n",
        "# Read the dataset\n",
        "file_path = '/content/english_telugu_data.txt'\n",
        "raw_data = read_telugu_english_data(file_path)\n",
        "train_data_raw, val_data_raw, test_data_raw = split_dataset(raw_data)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomDataset(train_data_raw)\n",
        "valid_dataset = CustomDataset(val_data_raw)\n",
        "test_dataset = CustomDataset(test_data_raw)\n",
        "\n",
        "# Load vocabularies\n",
        "vocab_src = build_vocabulary(tokenizer_te, train_dataset)\n",
        "vocab_trg = build_vocabulary(tokenizer_en, train_dataset)\n",
        "\n",
        "# Batch generation function\n",
        "def generate_batch(data_batch):\n",
        "    de_batch, en_batch = [], []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for (de_item, en_item) in data_batch:\n",
        "        # Convert list of indices into tensors\n",
        "        de_indices = torch.tensor([vocab_src[token] for token in tokenizer_te(de_item)], dtype=torch.long)\n",
        "        en_indices = torch.tensor([vocab_trg[token] for token in tokenizer_en(en_item)], dtype=torch.long)\n",
        "\n",
        "        # Concatenate BOS, indices, EOS\n",
        "        de_temp = torch.cat([torch.tensor([vocab_src['<bos>']], dtype=torch.long), de_indices, torch.tensor([vocab_src['<eos>']], dtype=torch.long)], dim=0).to(device)\n",
        "        en_temp = torch.cat([torch.tensor([vocab_trg['<bos>']], dtype=torch.long), en_indices, torch.tensor([vocab_trg['<eos>']], dtype=torch.long)], dim=0).to(device)\n",
        "\n",
        "        # Pad sequences to ensure consistent length\n",
        "        padded_de = F.pad(de_temp, (0, 20 - len(de_temp)), value=vocab_src['<pad>'])\n",
        "        padded_en = F.pad(en_temp, (0, 20 - len(en_temp)), value=vocab_trg['<pad>'])\n",
        "\n",
        "        de_batch.append(padded_de)\n",
        "        en_batch.append(padded_en)\n",
        "\n",
        "    return torch.stack(de_batch), torch.stack(en_batch)\n",
        "\n",
        "\n",
        "# DataLoader setup\n",
        "BATCH_SIZE = 128\n",
        "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "BOS_IDX = vocab_trg['<bos>']\n",
        "EOS_IDX = vocab_trg['<eos>']\n",
        "PAD_IDX = vocab_trg['<pad>']\n",
        "MAX_PADDING = 20\n",
        "BATCH_SIZE = 128\n"
      ],
      "metadata": {
        "id": "CvDPOoks2iG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Embedding Layer\n"
      ],
      "metadata": {
        "id": "oHgye3J-eYK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size: int, d_model: int):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size:    size of vocabulary\n",
        "      d_model:       dimension of embeddings\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.lut = nn.Embedding(vocab_size, d_model)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x:        input tensor (batch_size, sseq_lenght)\n",
        "\n",
        "      returns:  embedding vector\n",
        "    \"\"\"\n",
        "    return (self.lut(x) * math.sqrt(self.d_model))"
      ],
      "metadata": {
        "id": "1aokYT_-6Tmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n"
      ],
      "metadata": {
        "id": "0G0MuGsZhFVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:     dimension of embeddings\n",
        "      dropout:     randomly zeroes-out some of the input\n",
        "      max_lenght:  amx sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    pe=torch.zeros(max_length,d_model)\n",
        "    for k in np.arange(max_length):\n",
        "      for i in np.arange(d_model//2):\n",
        "        theta = k / (100** ((2*i)/d_model))\n",
        "\n",
        "\n",
        "        pe[k, 2*i] = math.sin(theta)\n",
        "\n",
        "\n",
        "        pe[k, 2*i+1] = math.cos(theta)\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x:        embeddings (batch_size, seq_lenght, d_model)\n",
        "      returns:  embeddings + positonal encodings (batch_size, seq_length, d_model)\n",
        "    \"\"\"\n",
        "    x = x + self.pe[:x.size(1)].requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "Ms26YAIahLAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "t3-3iKsYlPG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout: float = 0.1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:      dimension of embeddings\n",
        "      n_heads:      number of self attention heads\n",
        "      dropout:      probability of dropout occuring\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_key = d_model // n_heads\n",
        "\n",
        "    # create query, key, value, output weights\n",
        "    self.Wq = nn.Linear(d_model, d_model)\n",
        "    self.Wk = nn.Linear(d_model,d_model)\n",
        "    self.Wv = nn.Linear(d_model,d_model)\n",
        "    self.Wo = nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      query:    query vector (batch_size, q_length, d_model)\n",
        "      key:      key vector (batch_size, k_length, d_model)\n",
        "      value:    value vector (batch_size, s_length, d_model)\n",
        "      mask:     mask for decoder\n",
        "\n",
        "    Returns:\n",
        "      output:    attention values (batch_size, q_lenght, d_model)\n",
        "      attn_probs:  softmax scores (batchsize, n_heads, q_length, k_length)\n",
        "    \"\"\"\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # calculate query, key, and value tensors\n",
        "    Q = self.Wq(query)\n",
        "    K = self.Wk(key)          # (32, 10, 512) x (512, 512) = (32, 10, 512)\n",
        "    V = self.Wv(value)\n",
        "\n",
        "    # split each tensor into n_heads to compute attention\n",
        "\n",
        "    # query tensor\n",
        "    Q = Q.view(batch_size,\n",
        "               -1,                                    # (32, 10, 512) -> (32, 10, 8 ,64)\n",
        "               self.n_heads,                          # -1 = q_lenght\n",
        "               self.d_key).permute(0, 2, 1, 3)        # (32, 10, 8, 64) -> (32, 8, 10, 64)\n",
        "\n",
        "    # key tensor\n",
        "    K = K.view(batch_size,\n",
        "               -1,\n",
        "               self.n_heads,\n",
        "               self.d_key).permute(0, 2, 1, 3)\n",
        "\n",
        "    # value tensor\n",
        "    V = V.view(batch_size,\n",
        "               -1,\n",
        "               self.n_heads,\n",
        "               self.d_key).permute(0, 2, 1, 3)\n",
        "\n",
        "    # computes attention\n",
        "    # scalled dot product -> QK^{T}\n",
        "    scaled_dot_prod = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.d_key)\n",
        "\n",
        "    # fill thoes positions of product as (-1e10) where mask positions are 0\n",
        "    if mask is not None:\n",
        "      scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    attn_probs = torch.softmax(scaled_dot_prod, dim = -1)\n",
        "\n",
        "    # multiply by values to get attention\n",
        "    A = torch.matmul(self.dropout(attn_probs), V)\n",
        "\n",
        "\n",
        "    # reshape attention back to (32, 10, 512)\n",
        "    A = A.permute(0,2,1,3).contiguous()               # (32, 8, 10, 64) -> (32, 10, 8 ,64)\n",
        "    A = A.view(batch_size, -1, self.n_heads*self.d_key)     # (32, 10, 8, 64) -> (32, 10, 8*64) = (32, 10, 512)\n",
        "\n",
        "    output = self.Wo(A)\n",
        "\n",
        "    return output, attn_probs\n",
        "\n"
      ],
      "metadata": {
        "id": "BtfCR5etla3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position-Wise Feed Forward Network (FFN)\n"
      ],
      "metadata": {
        "id": "l2psFbWTmQzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ffn: int, dropout: float = 0.1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:      dimension of embeddings\n",
        "      d_ffn:        dimension of feed-forward network\n",
        "      dropout:      probability of dropout occuring\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    self.linear_layer_1 = nn.Linear(d_model, d_ffn)\n",
        "    self.linear_layer_2 = nn.Linear(d_ffn, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x:        output from attention (batch_size, seq_length, d_model)\n",
        "\n",
        "    Returns:\n",
        "      expanded-and-contracted representation (batch_size, seq_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    return self.linear_layer_2(self.dropout(self.linear_layer_1(x).relu()))\n",
        "\n"
      ],
      "metadata": {
        "id": "E_ervRegmTBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Encoder"
      ],
      "metadata": {
        "id": "9qt5k4houNYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:      dimension of embeddings\n",
        "      n_heads:      number of heads\n",
        "      d_ffn:        dimension of feed-forward network\n",
        "      dropout:      probability of dropout ocurring\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "    self.positionwise_fnn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
        "    self.fnn_layer_norm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src:      positionally embedded sequences (batch_size, seq_length, d_model)\n",
        "      src_mask: mask for the sequences (batch_size, 1, 1, seq_lenght)\n",
        "    Returns:\n",
        "      src:      Sequences after self-attention (batch_size, seq_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    _src, attn_probs = self.attention(src, src, src, src_mask)\n",
        "\n",
        "    src = self.attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    _src = self.positionwise_fnn(src)\n",
        "\n",
        "    src = self.fnn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    return src, attn_probs\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model: int, n_layers: int, n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:      dimension of embeddings\n",
        "      n_layers:     number of encoder layers\n",
        "      n_heads:      number of heads\n",
        "      d_ffn:        dimension of feed-forward network\n",
        "      dropout:      probability of dropout occuring\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # create n_layers encoders\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ffn, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src:      positionally embedded sequences (batch_size, seq_length, d_model)\n",
        "      src_mask: mask for the sequences (batch_size, 1, 1, seq_lenght)\n",
        "    Returns:\n",
        "      src:      Sequences after self-attention (batch_size, seq_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    # Pass the sequence through each encoder\n",
        "    for layer in self.layers:\n",
        "      src, attn_probs = layer(src, src_mask)\n",
        "\n",
        "    self.attn_probs = attn_probs\n",
        "    return src\n"
      ],
      "metadata": {
        "id": "zHWZpG2puQRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Decoder"
      ],
      "metadata": {
        "id": "K6aH0Ky28SDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ffn: int, dropout: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      d_model:      dimension of embeddings\n",
        "      n_heads:      number of heads\n",
        "      d_ffn:        dimension of feed-forward network\n",
        "      dropout:      probability of dropout occuring\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.masked_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.masked_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "    self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "    self.positionwise_fnn = PositionwiseFeedForward(d_model, d_ffn, dropout)\n",
        "    self.fnn_layer_norm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, trg, src, trg_mask, src_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      trg:          embedded sequences (batch_size, trg_seq_length, d_model)\n",
        "      src:          embedded sequences (batch_size, src_seq_length, d_model)\n",
        "      trg_mask:     mask for the sequences (batch_size, 1, trg_seq_length, trg_seq_lengt\n",
        "      src_mask:     mask for the sequences (batch_size, 1, 1, src_seq_length)\n",
        "\n",
        "    Returns:\n",
        "      trg: sequences after self-attention (batch_size, trg_seq_length, d_model)\n",
        "      attn_probs: self-attention softmax scores (batch_size, n_heads, trg_seq_length, src_seq_lenght)\n",
        "    \"\"\"\n",
        "\n",
        "    _trg, attn_probs = self.masked_attention(trg, trg, trg, trg_mask)\n",
        "\n",
        "    trg = self.masked_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg, attn_probs = self.attention(trg, src, src, src_mask)\n",
        "\n",
        "    trg = self.attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg = self.positionwise_fnn(trg)\n",
        "\n",
        "    trg = self.fnn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    return trg, attn_probs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ffn: int, dropout: float = 0.1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size:     size of the target vocabulary\n",
        "      d_model:        dimension of embeddings\n",
        "      n_layers:       number of encoder layers\n",
        "      n_heads:        number of heads\n",
        "      d_ffn:          dimension of feed-forward network\n",
        "      dropout:        probability of dropout occurring\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # create n_layers encoders\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ffn, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # set output layer\n",
        "    self.Wo = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, trg, src, trg_mask, src_mask):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        trg:          embedded sequences (batch_size, trg_seq_length, d_model)\n",
        "        src:          embedded sequences (batch_size, src_seq_length, d_model)\n",
        "        trg_mask:     mask for the sequences (batch_size, 1, trg_seq_length, trg_seq_lengt\n",
        "        src_mask:     mask for the sequences (batch_size, 1, 1, src_seq_length)\n",
        "\n",
        "      Returns:\n",
        "        output:       sequences after decoder (batch_size, trg_seq_length, vocab_size)\n",
        "        attn_probs:   self-attention softmax scores (batch_size, n_heads, trg_seq_length, src_seq\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      # pass the sequences through each decoder\n",
        "      for layer in self.layers:\n",
        "        trg , attn_probs = layer(trg, src, trg_mask, src_mask)\n",
        "\n",
        "      self.attn_probs = attn_probs\n",
        "      return self.Wo(trg)\n",
        "\n"
      ],
      "metadata": {
        "id": "rvErbeGT_d5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer"
      ],
      "metadata": {
        "id": "JN8JAYszwF-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: Embeddings,\n",
        "               trg_embed: Embeddings, src_pad_idx: int, trg_pad_idx: int, device):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      encoder:        encoder stack\n",
        "      decoder:        decoder stack\n",
        "      src_embed:      source embeddings and encodings\n",
        "      trg_embed:      target embeddings and encodings\n",
        "      src_pad_idx:    padding index\n",
        "      trg_pad_idx:    padding index\n",
        "      device:         cpu or gpu\n",
        "\n",
        "    Returns:\n",
        "      output:         sequences after decoder (batch_size, trg_seq_length, vocab_size)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.device = device\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src:        raw sequence with padding     (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "      src_mask:   mask for each sequence        (batch_size, 1, 1, seq_lenght)\n",
        "    \"\"\"\n",
        "    # assign 1 to tokens that need attended to and 0 to padding tokens, then add 2 dimensions\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      trg:        raw sequence with padding     (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "      trg_mask:   mask for each sequence        (batch_size, 1, seq_length, seq_lenght)\n",
        "    \"\"\"\n",
        "    seq_length = trg.shape[1]\n",
        "\n",
        "    # assign True to tokens that need attended to and False to padding tokens, then add 2 dimensions\n",
        "    trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # generate subsequent mask\n",
        "    trg_sub_mask = torch.tril(torch.ones((seq_length, seq_length), device = self.device )).bool()\n",
        "\n",
        "    # bitwise \"and\" operator\n",
        "    trg_mask = trg_mask & trg_sub_mask\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    \"\"\"\n",
        "    Args\n",
        "      trg:        raw target sequence (batch_size, trg_seq_length)\n",
        "      src:        raw src sequences (batch_size, src_seq_length)\n",
        "\n",
        "    Returns:\n",
        "      output:     sequences after decoder   (batch_size, trg_seq_length, output_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    # create source and target masks\n",
        "    src_mask = self.make_src_mask(src)    #(batch_size, 1, 1, src_seq_length)\n",
        "    trg_mask = self.make_trg_mask(trg)    #(batch_size, 1, trg_seq_length, trg_seq_length)\n",
        "\n",
        "    # push the src through the encoder layers\n",
        "    src = self.encoder(self.src_embed(src), src_mask)   # (batch_size, src_seq_length, d_model)\n",
        "\n",
        "    # decoder output and attention probabilities\n",
        "    output = self.decoder(self.trg_embed(trg), src, trg_mask, src_mask)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Ia5Qpg8pwH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating a Model"
      ],
      "metadata": {
        "id": "uryJSF4d4tQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(device, src_vocab, trg_vocab, n_layers: int = 3, d_model: int = 256,\n",
        "               d_ffn: int = 2048, n_heads: int = 8, dropout: float = 0.1,\n",
        "               max_length: int = 5000):\n",
        "  \"\"\"\n",
        "    Construct a model when provided parameters.\n",
        "\n",
        "    Args:\n",
        "      src_vocab:      source vocubulary\n",
        "      trg_vocab:      target vocubulary\n",
        "      n_layers:       Number of encoder and decoders\n",
        "      d_model:        dimension of embeddinsg\n",
        "      d_ffn:          dimension of feed-forwaed network\n",
        "      n_heads:        number of heads\n",
        "      dropout:        probability of dropout ocurring\n",
        "      max_length:     maximum sequence length for positional encodings\n",
        "\n",
        "    Returns:\n",
        "      Transformer model based on hyperparameters\n",
        "  \"\"\"\n",
        "  encoder = Encoder(d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "  decoder = Decoder(len(trg_vocab), d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "  src_embed = Embeddings(len(src_vocab), d_model)\n",
        "  trg_embed = Embeddings(len(trg_vocab), d_model)\n",
        "  pos_enc = PositionalEncoding(d_model, dropout, max_length)\n",
        "\n",
        "  # create the Transformer model\n",
        "  model = Transformer(encoder, decoder, nn.Sequential(src_embed, pos_enc),\n",
        "                      nn.Sequential(trg_embed, pos_enc),\n",
        "                      src_pad_idx = src_vocab.get_stoi()[\"<pad>\"],\n",
        "                      trg_pad_idx = trg_vocab.get_stoi()[\"<pad>\"],\n",
        "                      device = device)\n",
        "\n",
        "  # initialize parameters with Xavier/Glorot\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "tlJT1JTn4Q0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing the data\n",
        "\n"
      ],
      "metadata": {
        "id": "gSE2w7hvFTtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/english_telugu_data.txt', 'r', encoding='utf-8') as file:\n",
        "  raw = []\n",
        "  n = 0\n",
        "  for line in file:\n",
        "    n +=1\n",
        "    if n==5:\n",
        "      break\n",
        "    telugu_sentence, english_sentence = line.strip().split('++++$++++')\n",
        "    raw.append((telugu_sentence, english_sentence))\n",
        "\n",
        "print(raw)\n"
      ],
      "metadata": {
        "id": "NF2JtA-c5O7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f707737b-7ab2-48f4-d2e5-70dea93c74bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('His legs are long.', 'అతని కాళ్ళు పొడవుగా ఉన్నాయి.'), ('Who taught Tom how to speak French?', 'టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?'), ('I swim in the sea every day.', 'నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.'), ('Tom popped into the supermarket on his way home to buy some milk.', 'టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడు సూపర్ మార్కెట్లోకి ప్రవేశించాడు.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = make_model(device, vocab_src, vocab_trg,\n",
        "                   n_layers=3, n_heads=8, d_model=256,\n",
        "                   d_ffn=512, max_length=50)\n",
        "\n",
        "model.cuda()\n"
      ],
      "metadata": {
        "id": "eXh9Uq5OImJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6766eee2-d723-4637-a7bf-717f68e63d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (positionwise_fnn): PositionwiseFeedForward(\n",
              "          (linear_layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (linear_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (fnn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x DecoderLayer(\n",
              "        (masked_attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (masked_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): MultiHeadAttention(\n",
              "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (Wo): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (positionwise_fnn): PositionwiseFeedForward(\n",
              "          (linear_layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (linear_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (fnn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (Wo): Linear(in_features=256, out_features=25280, bias=True)\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(25280, 256)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (trg_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(25280, 256)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "ubvR7SYG8UpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef5eabe-4151-4e15-df25-f59b4707d700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 23,393,984 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "metadata": {
        "id": "oPFLBs6x80yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "  # set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  # loop through each batch in the iterator\n",
        "  for i, batch in enumerate(iterator):\n",
        "\n",
        "    # set the source and target batches\n",
        "    src,trg = batch\n",
        "\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # logits for each output\n",
        "    logits = model(src, trg[:,:-1])\n",
        "\n",
        "    # expected output\n",
        "    expected_output = trg[:,1:]\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                    expected_output.contiguous().view(-1))\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the weights\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    # update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # update the loss\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # return the average loss for the epoch\n",
        "  return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "P-vPDaiF81xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  # set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  # evaluate without updating gradients\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # loop through each batch in the iterator\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "      # set the source and target batches\n",
        "      src, trg = batch\n",
        "\n",
        "\n",
        "      # logits for each output\n",
        "      logits = model(src, trg[:,:-1])\n",
        "\n",
        "      # expected output\n",
        "      expected_output = trg[:,1:]\n",
        "\n",
        "      # calculate the loss\n",
        "      loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                      expected_output.contiguous().view(-1))\n",
        "\n",
        "      # update the loss\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  # return the average loss for the epoch\n",
        "  return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "ENJopwcw9BAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "D1Z3UPwp9J5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dBBhFjBxgC36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b9a5e5-76f1-44a4-e607-5815c7aa7854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 12 16:45:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0              31W /  70W |    197MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS =10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# loop through each epoch\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # calculate the train loss and update the parameters\n",
        "  train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "\n",
        "  # calculate the loss on the validation set\n",
        "  valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  # calculate how long the epoch took\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  # save the model when it performs better than the previous run\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'transformer-model_tel.pt')\n",
        "\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oIhMktB5nuL",
        "outputId": "91f5e657-9d0a-4b8b-ff57-c6eb94a60b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 30s\n",
            "\tTrain Loss: 4.020 | Train PPL:  55.702\n",
            "\t Val. Loss: 2.700 |  Val. PPL:  14.884\n",
            "Epoch: 02 | Time: 1m 30s\n",
            "\tTrain Loss: 2.369 | Train PPL:  10.689\n",
            "\t Val. Loss: 1.763 |  Val. PPL:   5.830\n",
            "Epoch: 03 | Time: 1m 31s\n",
            "\tTrain Loss: 1.613 | Train PPL:   5.019\n",
            "\t Val. Loss: 1.354 |  Val. PPL:   3.871\n",
            "Epoch: 04 | Time: 1m 31s\n",
            "\tTrain Loss: 1.185 | Train PPL:   3.272\n",
            "\t Val. Loss: 1.118 |  Val. PPL:   3.058\n",
            "Epoch: 05 | Time: 1m 30s\n",
            "\tTrain Loss: 0.932 | Train PPL:   2.539\n",
            "\t Val. Loss: 1.023 |  Val. PPL:   2.781\n",
            "Epoch: 06 | Time: 1m 30s\n",
            "\tTrain Loss: 0.767 | Train PPL:   2.154\n",
            "\t Val. Loss: 0.950 |  Val. PPL:   2.586\n",
            "Epoch: 07 | Time: 1m 30s\n",
            "\tTrain Loss: 0.656 | Train PPL:   1.926\n",
            "\t Val. Loss: 0.916 |  Val. PPL:   2.500\n",
            "Epoch: 08 | Time: 1m 30s\n",
            "\tTrain Loss: 0.575 | Train PPL:   1.777\n",
            "\t Val. Loss: 0.903 |  Val. PPL:   2.468\n",
            "Epoch: 09 | Time: 1m 31s\n",
            "\tTrain Loss: 0.513 | Train PPL:   1.671\n",
            "\t Val. Loss: 0.891 |  Val. PPL:   2.437\n",
            "Epoch: 10 | Time: 1m 31s\n",
            "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
            "\t Val. Loss: 0.883 |  Val. PPL:   2.419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the weights\n",
        "model.load_state_dict(torch.load('transformer-model_tel.pt'))\n",
        "\n",
        "# calculate the loss on the test set\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
      ],
      "metadata": {
        "id": "EuQ2ULtR9Vww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a34c53-6cdf-43d0-8cc9-e8e01b954ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.881 | Test PPL:   2.414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def translate_sentence(sentence, model, device, vocab_src, vocab_trg, tokenizer_te, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Check if the input is a string and tokenize accordingly\n",
        "    if isinstance(sentence, str):\n",
        "        # Tokenize the sentence using the Telugu tokenizer\n",
        "        tokens = tokenizer_te(sentence)\n",
        "        src = ['<bos>'] + [token.lower() for token in tokens] + ['<eos>']\n",
        "    else:\n",
        "        src = ['<bos>'] + sentence + ['<eos>']\n",
        "\n",
        "    # Map the tokens to their respective indices in the source vocabulary\n",
        "    src_indexes = [vocab_src[token] if token in vocab_src else vocab_src['<unk>'] for token in src]\n",
        "\n",
        "    # Convert the list of indices to a tensor and add a batch dimension\n",
        "    src_tensor = torch.tensor(src_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the list of target indices with the index of '<bos>'\n",
        "    trg_indexes = [vocab_trg['<bos>']]\n",
        "\n",
        "    # Initialize the loop to generate tokens up to a maximum length\n",
        "    for i in range(max_length):\n",
        "        # Convert the current list of target indices to a tensor and add a batch dimension\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Feed the source and target tensors to the model to get the logits\n",
        "            output = model(src_tensor, trg_tensor)\n",
        "            pred_token = output.argmax(2)[:, -1].item()\n",
        "\n",
        "            # Check if the predicted token is '<eos>' or the maximum length is reached\n",
        "            if pred_token == vocab_trg['<eos>'] or i == (max_length - 1):\n",
        "                # Convert indices to tokens\n",
        "                trg_tokens = [vocab_trg.lookup_token(index) for index in trg_indexes[1:]]  # Skip '<bos>'\n",
        "                return src, trg_tokens\n",
        "\n",
        "            # Append the predicted token to the list of target indices\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "# Example usage\n",
        "src_text = \"how was it?\"\n",
        "model = model  # Replace with your actual model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Assuming CUDA is available and appropriate\n",
        "src, trg_tokens = translate_sentence(src_text, model, device, vocab_src, vocab_trg, tokenizer_te)\n",
        "print(f'source --> {\" \".join(src[1:-1])}')\n",
        "print(f'target translation --> {\" \".join(trg_tokens)}')\n"
      ],
      "metadata": {
        "id": "2HAJ11yE9NRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa86a42a-7a0b-4397-f454-3e1c3e035141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source --> how was it ?\n",
            "target translation --> అది ఎలా ఉంది ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = \"The technology is increasing rapidly\"\n",
        "src, trg_tokens = translate_sentence(src_text, model, device, vocab_src, vocab_trg, tokenizer_te)\n",
        "print(f'source --> {\" \".join(src[1:-1])}')\n",
        "print(f'target translation --> {\" \".join(trg_tokens)}')"
      ],
      "metadata": {
        "id": "DFklh6Vo9XaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2550c5d3-8793-474e-d8ad-256985095741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source --> the technology is increasing rapidly\n",
            "target translation --> సాంకేతిక సాంకేతిక శక్తి వేగంగా పెరుగుతోంది .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = \"i like to learn new things\"\n",
        "src, trg_tokens = translate_sentence(src_text, model, device, vocab_src, vocab_trg, tokenizer_te)\n",
        "print(f'source --> {\" \".join(src[1:-1])}')\n",
        "print(f'target translation --> {\" \".join(trg_tokens)}')"
      ],
      "metadata": {
        "id": "avCdryzulxSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df690a43-7b66-468f-d44a-4e00dc07dc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source --> i like to learn new things\n",
            "target translation --> నేను క్రొత్త పనులు నేర్చుకోవడం ఇష్టం .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fc3KRDJ2MK7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}